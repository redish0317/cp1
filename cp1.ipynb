{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNv6E6nTySEXDFuCsUxD1U4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/redish0317/cp1/blob/main/cp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC5xjICdkTuu"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def crawl_id(limit=100,offset=0):\n",
        "    url = 'http://www.wanted.jobs/api/v4/jobs?'\n",
        "\n",
        "    params ={1656232918453:'',    #사용자번호?\n",
        "    'country': 'all',\n",
        "    'tag_type_ids': 873,    #직무 카테고리 고유 id\n",
        "    'job_sort': 'job.latest_order',    #최신순 정렬\n",
        "    'locations': all,\n",
        "    'years': -1,    #경력 이상\n",
        "    'years': -1,    #경력 이하    경력상관없이 검색하려면 -1\n",
        "    'limit': limit,    #한 번에 조회 가능한 수 (최대100)\n",
        "    'offset': offset}    #조회할 게시물의 첫 index        ex) limit=100 offset=10  => 10번게시물부터 110번게시물까지 크롤링\n",
        "\n",
        "    #서버에 url과 쿼리로 요청\n",
        "    r = requests.get(url,\n",
        "                     params = params)\n",
        "    #요청한 데이터 json포멧으로 변환\n",
        "    r = r.json()\n",
        "    #json포멧 데이터중 id컬럼만 추출\n",
        "    id_list = [i['id'] for i in r['data']]\n",
        "    return id_list\n",
        "\n",
        "def return_id_list():\n",
        "    '''\n",
        "    0번째 게시물부터 100개씩 크롤링 while true\n",
        "    오류발생! => ex) 총 게시물이 321개인데 300개 크롤링 후 다음100개를 크롤링하려했기때문\n",
        "    따라서 재귀호출을 통해 크롤링 수를 100개씩 -> 오류발생! -> 10개씩 -> 오류발생! -> 1개씩 크롤링하는 함수구현\n",
        "    만약 게시물이 321개라면 300개 크롤링 -> 20개 크롤링 - 1개 크롤링 return\n",
        "    '''\n",
        "    id_list=[]\n",
        "    def crawl_all_id(limit=100,offset=0):\n",
        "        try:\n",
        "            while True:\n",
        "                id_list.extend(crawl_id(limit,offset))\n",
        "                offset+=limit\n",
        "\n",
        "        except:\n",
        "            if limit != 1:\n",
        "                return crawl_all_id(limit/10,offset)\n",
        "    crawl_all_id()\n",
        "    return id_list\n",
        "\n",
        "\n",
        "def crawl_job(id_list):\n",
        "    df_list = []\n",
        "    \n",
        "    for id in id_list:\n",
        "        url = f'https://www.wanted.jobs/api/v4/jobs/{id}?1656259528432'\n",
        "        r = requests.get(url)\n",
        "        r = r.json()['job']\n",
        "\n",
        "        #1개의 게시물 크롤링할때마다 데이터프레임에 append 또는 concat하는것보다\n",
        "        #list에 append하고 마지막에 한번에 concat하는게 속도가 더 빠르다고 함\n",
        "        df_list.append(pd.json_normalize(r))\n",
        "        \n",
        "    df = pd.concat(df_list, ignore_index=True)\n",
        "    #단일 게시물을 크롤링해서 concat하다보니 index가 모두 0이므로 reset_index\n",
        "    return df\n",
        "\n",
        "\n",
        "def engineering(df):\n",
        "    drop_col = []\n",
        "    df = df.drop(drop_col,axis=1)\n",
        "    return df\n",
        "start = time.time()\n",
        "\n",
        "id_list = return_id_list()\n",
        "end=time.time()\n",
        "print(end-start)\n",
        "\n",
        "df_job = crawl_job(id_list)\n",
        "end=time.time()\n",
        "print(end-start)"
      ]
    }
  ]
}